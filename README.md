# tsne-mlx

t-SNE in pure MLX for Apple Silicon. Entire pipeline runs on Metal GPU.

5-12x faster than sklearn. Fashion-MNIST 70K in 4.9 seconds.



https://github.com/user-attachments/assets/PLACEHOLDER_VIDEO



## Install

```bash
git clone https://github.com/hanxiao/tsne-mlx.git && cd tsne-mlx
uv venv .venv && source .venv/bin/activate
uv pip install -e .
```

## Usage

```python
from tsne_mlx import TSNE
import numpy as np

X = np.random.randn(70000, 784).astype(np.float32)
Y = TSNE(n_components=2, perplexity=30, n_iter=750).fit_transform(X)
```

Parameters:

- `n_components`: output dimensions (default 2)
- `perplexity`: effective number of neighbors (default 30)
- `learning_rate`: gradient descent learning rate (default 200)
- `n_iter`: optimization iterations (default 1000)
- `early_exaggeration`: P multiplier during early phase (default 12)
- `early_exaggeration_iter`: early exaggeration duration (default 250)
- `random_state`: seed for reproducibility
- `verbose`: print progress every N iterations (0 = silent)
- `pca_dim`: PCA preprocessing dimension (default 50, None to disable)
- `epoch_callback`: optional `callable(epoch, Y_numpy)` for animation snapshots

## Performance (M3 Ultra, Fashion-MNIST)

```
N        sklearn     tsne-mlx    speedup
10000    10.3s       6.2s        1.7x
70000    60.7s       4.9s        12x
```

Small N uses compiled exact repulsive forces on GPU. Large N (>16K) switches to
FFT-accelerated interpolation (FIt-SNE algorithm) for O(n) repulsive computation.

## Comparison

Fashion-MNIST 10K (784 dims, 10 classes, 1000 iterations):

![comparison](comparison.png?raw=true)

## How it works

1. PCA to 50 dims (when input dim > 50)
2. Chunked brute-force KNN on Metal GPU (3 * perplexity neighbors)
3. Vectorized binary search for per-point bandwidth (all N points on GPU)
4. Symmetrized sparse P matrix via GPU argsort + searchsorted
5. Gradient descent with momentum (0.5/0.8) and adaptive gains:
   - Attractive: sparse KNN edges weighted by joint probability
   - Repulsive (N < 16K): compiled exact all-pairs via matmul trick on GPU
   - Repulsive (N >= 16K): FFT interpolation -- Lagrange scatter to grid, batched FFT convolution with Cauchy kernel, gather back
6. PCA initialization (first 2 components, scaled to 1e-4)

Dependencies: `mlx` and `numpy` only. No scipy, no PyTorch.

## Animation

The demo video above was generated by [`fashion_mnist_anim.py`](fashion_mnist_anim.py).

## See also

- [umap-mlx](https://github.com/hanxiao/umap-mlx) -- UMAP in pure MLX. Fuzzy simplicial set + negative sampling SGD. 30x faster than umap-learn.
- [pacmap-mlx](https://github.com/hanxiao/pacmap-mlx) -- PaCMAP in pure MLX. Near/mid-near/far pairs with phase-scheduled weights. Better global structure preservation.

## License

MIT
